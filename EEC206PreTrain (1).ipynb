{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJpk_TTySw_e",
        "outputId": "b1ec57ef-0cc9-4311-c922-fb587c52a908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "### Cell to link Notebook to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e_bfDdQfIfa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sklearn\n",
        "import statistics\n",
        "import scipy\n",
        "import PIL\n",
        "from skimage import io, transform\n",
        "import random\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy import interp\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from scipy.interpolate import CubicSpline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF7bV4Ezxf12"
      },
      "outputs": [],
      "source": [
        "class PreMendeleyTrain_Dataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform = None, transform_gt=None,isSeg = False):\n",
        "    \n",
        "    self.data_amt = 0\n",
        "    self.path = root_dir\n",
        "    self.isSeg = isSeg\n",
        "    self.transform = transform\n",
        "    self.transform_gt = transform_gt\n",
        "\n",
        "    self.X = {}\n",
        "    self.Y = {}\n",
        "\n",
        "    self.grab_data()\n",
        "\n",
        "  def grab_data(self):\n",
        "    for species in sorted(os.listdir(self.path)):\n",
        "      diseased_path = os.path.join(self.path,species,'diseased')\n",
        "      healthy_path = os.path.join(self.path,species,'healthy')\n",
        "      \n",
        "      if (os.path.exists(diseased_path)):\n",
        "        for image in sorted(os.listdir(diseased_path)):\n",
        "          self.X[diseased_path+'/'+image] = diseased_path+'/'+image\n",
        "          label = self.name2label(species)\n",
        "          self.data_amt += 1\n",
        "          \n",
        "          self.Y[diseased_path+'/'+image] = label\n",
        "\n",
        "      if (os.path.exists(healthy_path)):\n",
        "        for image in sorted(os.listdir(healthy_path)):\n",
        "          self.X[healthy_path+'/'+image] = healthy_path+'/'+image\n",
        "          label = self.name2label(species)\n",
        "          self.data_amt += 1\n",
        "          \n",
        "          self.Y[healthy_path+'/'+image] = label\n",
        "\n",
        "  def name2label(self,name):\n",
        "    species = sorted(os.listdir(self.path))\n",
        "    label = species.index(name)\n",
        "\n",
        "    return label\n",
        "      \n",
        "  def __len__(self):\n",
        "    return self.data_amt\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dict_names = list(self.X.keys())\n",
        "    seg_dict_names = list(self.Y.keys())\n",
        "\n",
        "    dict_name = dict_names[idx]\n",
        "    seg_dict_name = seg_dict_names[idx]\n",
        "    \n",
        "\n",
        "    image = PIL.Image.open(dict_name)\n",
        "\n",
        "    if self.isSeg:\n",
        "\n",
        "      gt = PIL.Image.open(seg_dict_name)\n",
        "    else:\n",
        "      gt = self.Y[dict_name]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if self.isSeg:\n",
        "      gt = self.transform_gt(gt)\n",
        "\n",
        "    sample = {'segment': image, 'gt': gt}\n",
        "    \n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drchu84NTIid"
      },
      "outputs": [],
      "source": [
        "class PreTrain_Dataset(Dataset):\n",
        "  def __init__(self, path, transform = None, transform_gt=None,isSeg = False):\n",
        "    \n",
        "    self.data_amt = 0\n",
        "    self.path = path\n",
        "    self.isSeg = isSeg\n",
        "    self.transform = transform\n",
        "    self.transform_gt = transform_gt\n",
        "\n",
        "    self.X = {}\n",
        "    self.Y = {}\n",
        "\n",
        "    self.grab_data()\n",
        "\n",
        "  def grab_data(self):\n",
        "    for local in os.listdir(self.path+'images/'):\n",
        "      local_path = self.path+'images/' + local + '/'\n",
        "      seg_path = self.path+'segmented/' + local + '/'\n",
        "      for species in os.listdir(local_path):\n",
        "        folder_path = local_path + species + '/'\n",
        "        seg_path = seg_path + species + '/'\n",
        "        for images in os.listdir(folder_path):\n",
        "          #print(images)\n",
        "          #self.X[folder_path+images] = PIL.Image.open(folder_path+images)\n",
        "          self.X[folder_path+images] = folder_path+images\n",
        "          label = self.name2label(species)\n",
        "          self.data_amt += 1\n",
        "          if self.isSeg:\n",
        "            #self.Y[folder_path+images] = PIL.Image.open(seg_path+images)\n",
        "            self.Y[folder_path+images] = seg_path+images\n",
        "          else:\n",
        "            self.Y[folder_path+images] = label\n",
        "\n",
        "  def name2label(self,name):\n",
        "    species = sorted(os.listdir(self.path+'images/lab/'))\n",
        "    label = species.index(name)\n",
        "\n",
        "    return label\n",
        "      \n",
        "  def __len__(self):\n",
        "    return self.data_amt\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dict_names = list(self.X.keys())\n",
        "    seg_dict_names = list(self.Y.keys())\n",
        "\n",
        "    dict_name = dict_names[idx]\n",
        "    seg_dict_name = seg_dict_names[idx]\n",
        "\n",
        "    image = PIL.Image.open(dict_name)\n",
        "\n",
        "    if self.isSeg:\n",
        "      #gt = io.imread(seg_dict_names[idx])\n",
        "      gt = PIL.Image.open(seg_dict_name)\n",
        "    else:\n",
        "      gt = self.Y[dict_name]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "      #sample = self.transform(sample)\n",
        "\n",
        "    if self.isSeg:\n",
        "      gt = self.transform_gt(gt)\n",
        "\n",
        "    #gt = torchvision.transforms.functional.to_tensor(gt)\n",
        "\n",
        "    sample = {'segment': image, 'gt': gt}\n",
        "    \n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJqBby52TIc_"
      },
      "outputs": [],
      "source": [
        "#### Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q27kDC8hTIav"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3,16,3,padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16,32,5,padding = 2,stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32,32,3,padding = 1,stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.GAP = nn.AvgPool2d(1)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.bn1(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        \n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.bn3(out)\n",
        "        \n",
        "        #print(out.size())\n",
        "        \n",
        "        out = self.GAP(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "### TO VINCENT: Here is the classification module for the network. Change the last linear layer size to the number of classes in your dataset\n",
        "class LeafSnap_Classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeafSnap_Classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(75*75*32,512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        ### Here the 185 is the amount of classes in LeafSnap dataset\n",
        "        self.fc2 = nn.Linear(512,185)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = x.view(x.size(0), -1)\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Mendeley_Classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mendeley_Classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(75*75*32,512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512,12) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = x.view(x.size(0), -1)\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bali_Classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bali_Classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(75*75*32,512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512,26)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.conv1 = nn.ConvTranspose2d(32,32,3,padding = 1,stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.ConvTranspose2d(32,16,5,padding = 2,stride=2,output_padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv3 = nn.ConvTranspose2d(16,1,3)\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.bn1(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        \n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Leaf_FCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Leaf_FCN, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder()\n",
        "        \n",
        "        self.leafsnap = LeafSnap_Classification()\n",
        "        self.main = Bali_Classification()\n",
        "        self.mendelev = Mendeley_Classification()\n",
        "\n",
        "        self.decoder = Decoder()\n",
        "        \n",
        "    def forward(self, x, isMain = True, isSnap = False, isSeg = False, isMend = False):\n",
        "        if isSeg:\n",
        "            result = self.decoder(self.encoder(x))\n",
        "        else:\n",
        "            if isMain:\n",
        "              result = self.main(self.encoder(x))\n",
        "            elif isSnap:\n",
        "              result = self.leafsnap(self.encoder(x))\n",
        "            elif isMend:\n",
        "              result = self.mendelev(self.encoder(x))\n",
        "            \n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePx4qgGMTIYK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIqPmI7QTIVd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVadTR2AZWvt"
      },
      "outputs": [],
      "source": [
        "def randomInit(m):\n",
        "    print(\"Model Randomly Initialized\")\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "        \n",
        "\n",
        "def Joint_Loss(leafsnap_loss, leafsnap_seg_loss, mendelev_loss,mu=1):\n",
        "    if leafsnap_seg_loss == None:\n",
        "        loss = leafsnap_loss\n",
        "    elif leafsnap_loss == None:\n",
        "        loss = leafsnap_seg_loss\n",
        "    else:\n",
        "      if mendelev_loss == None:\n",
        "        loss = mu*leafsnap_seg_loss + leafsnap_loss\n",
        "      else:\n",
        "        loss = mu*leafsnap_seg_loss + leafsnap_loss + mendelev_loss*mu*2\n",
        "    return loss\n",
        "\n",
        "\n",
        "def fit(model, loss1, loss2,loss3, optimizer, train_loader,seg_loader,mend_loader, val_loader, batch_size, num_epochs, scheduler = None, stat_count=100, device=None,num_ch=3,stepsize=10,PATH = '/content/drive/MyDrive/EEC205/FCN_std_path.pt'):\n",
        "    less_losses = []\n",
        "    all_losses = []\n",
        "    train_acc = []\n",
        "    lastest_train_acc = []\n",
        "    loss_epoch = []\n",
        "    epoch_count = 0\n",
        "\n",
        "    curr_model_score = 0\n",
        "\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/EEC205/resnet18_final_pt.pt')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    model.fc = nn.Linear(512,26)\n",
        "\n",
        "    if device is not None:\n",
        "        model.to(device)\n",
        "    else:\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "    randomInit(model)\n",
        "\n",
        "    if scheduler == None:\n",
        "        scheduler = StepLR(optimizer, step_size=stepsize, gamma=0.1)\n",
        "    \n",
        "    num_steps = len(train_loader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        loss_u = None\n",
        "        if epoch != 0:\n",
        "            scheduler.step()\n",
        "            loss_epoch.append(f_loss.item())\n",
        "            \n",
        "        for train_ct in range(num_steps):\n",
        "            \n",
        "            try:\n",
        "                data = labelled_iter.next()\n",
        "            except:\n",
        "                labelled_iter = iter(train_loader)\n",
        "                data = labelled_iter.next()\n",
        "\n",
        "            try:\n",
        "                data_seg = seg_iter.next()\n",
        "            except:\n",
        "                seg_iter = iter(seg_loader)\n",
        "                data_seg = seg_iter.next()\n",
        "\n",
        "            try:\n",
        "                data_mend = mend_iter.next()\n",
        "            except:\n",
        "                mend_iter = iter(mend_loader)\n",
        "                data_mend = mend_iter.next()\n",
        "\n",
        "            with torch.enable_grad():\n",
        "                model.train()\n",
        "                # labeled data\n",
        "                images, labels = data[0].to(device,dtype=torch.float), data[1].to(device,dtype=torch.long)\n",
        "                #images = torch.reshape(images, (batch_size,num_ch,input_size))\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                outputs = outputs.float()\n",
        "                #print(labels)\n",
        "                loss = loss1(outputs, labels)\n",
        "\n",
        "                images_seg, labels_seg = data_seg['segment'].to(device,dtype=torch.float), data_seg['gt'].to(device,dtype=torch.float)\n",
        "                outputs_seg = model(images_seg, isMain=False,isSeg=True,isMend=False,isSnap=False)\n",
        "                outputs_seg = outputs_seg.float()\n",
        "                loss_seg = loss2(outputs_seg, labels_seg)\n",
        "\n",
        "                images_mend, labels_mend = data_mend['segment'].to(device,dtype=torch.float), data_mend['gt'].to(device,dtype=torch.long)\n",
        "                outputs_mend = model(images_mend, isMain=False,isSeg=False,isMend=True)\n",
        "                outputs_mend = outputs_mend.float()\n",
        "                loss_mend = loss3(outputs_mend, labels_mend)\n",
        "\n",
        "\n",
        "                f_loss = Joint_Loss(loss,loss_seg,loss_mend,mu=0.25)\n",
        "                f_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                \n",
        "                all_losses.append(f_loss.item())\n",
        "\n",
        "                # Print statistics on every stat_count iteration\n",
        "                if (train_ct+1) % stat_count == 0:\n",
        "                    less_losses.append(f_loss.item())\n",
        "                    print('Epoch [%d/%d], Step [%d/%d],  Loss: %.4f'\n",
        "                                %(epoch+1, num_epochs, train_ct+1, \n",
        "                                len(train_loader), f_loss.item()))\n",
        "\n",
        "        #end of batch for loop\n",
        "\n",
        "        #print(\"Forced Save\")\n",
        "        #torch.save({'model_state_dict': model.state_dict()}, PATH)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            total_train = 0\n",
        "            total_len = 0\n",
        "            total_lengood = 0\n",
        "            total_correct = 0\n",
        "            train_predicted_full = []\n",
        "            train_labels_full = []\n",
        "            for val_data in val_loader:\n",
        "                valimages, vallabels = val_data[0].cuda(), val_data[1].cuda()\n",
        "                valimages = valimages.float()\n",
        "                #vallabels = vallabels.float()\n",
        "\n",
        "                #valimages = torch.reshape(valimages, (batch_size,num_ch,input_size))\n",
        "                trainoutputs = model(valimages)\n",
        "\n",
        "                _, trainpredicted = torch.max(trainoutputs.data, 1)\n",
        "                total_train += vallabels.size(0)\n",
        "                total_correct += (trainpredicted == vallabels).sum().item()\n",
        "\n",
        "                train_predicted_full = train_predicted_full + trainpredicted.cpu().data.numpy().tolist()\n",
        "                train_labels_full = train_labels_full +vallabels.cpu().data.numpy().tolist()\n",
        "            #end of train eval\n",
        "        #current epoch for loop          \n",
        "        print(\"END OF EPOCH\")\n",
        "        class_dict = classification_report(train_labels_full, train_predicted_full, labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],output_dict=True)\n",
        "        score_0 = class_dict['0']['f1-score'] + 0.0001\n",
        "        score_1 = class_dict['1']['f1-score'] + 0.0001\n",
        "        score_2 = class_dict['2']['f1-score'] + 0.0001\n",
        "        score_3 = class_dict['3']['f1-score'] + 0.0001\n",
        "        score_4 = class_dict['4']['f1-score'] + 0.0001\n",
        "        score_5 = class_dict['5']['f1-score'] + 0.0001\n",
        "        score_6 = class_dict['6']['f1-score'] + 0.0001\n",
        "        score_7 = class_dict['7']['f1-score'] + 0.0001\n",
        "        score_8 = class_dict['8']['f1-score'] + 0.0001\n",
        "        score_9 = class_dict['9']['f1-score'] + 0.0001\n",
        "        score_10 = class_dict['10']['f1-score'] + 0.0001\n",
        "        score_11 = class_dict['11']['f1-score'] + 0.0001\n",
        "        score_12 = class_dict['12']['f1-score'] + 0.0001\n",
        "        score_13 = class_dict['13']['f1-score'] + 0.0001\n",
        "        score_14 = class_dict['14']['f1-score'] + 0.0001\n",
        "        score_15 = class_dict['15']['f1-score'] + 0.0001\n",
        "        score_16 = class_dict['16']['f1-score'] + 0.0001\n",
        "        score_17 = class_dict['17']['f1-score'] + 0.0001\n",
        "        score_18 = class_dict['18']['f1-score'] + 0.0001\n",
        "        score_19 = class_dict['19']['f1-score'] + 0.0001\n",
        "        score_20 = class_dict['20']['f1-score'] + 0.0001\n",
        "        score_21 = class_dict['21']['f1-score'] + 0.0001\n",
        "        score_22 = class_dict['22']['f1-score'] + 0.0001 \n",
        "        score_23 = class_dict['23']['f1-score'] + 0.0001\n",
        "        score_24 = class_dict['24']['f1-score'] + 0.0001\n",
        "        score_25 = class_dict['25']['f1-score'] + 0.0001\n",
        "        score = score_0 * score_1 * score_2 * score_3 * score_4 * score_5 * score_6 * score_7 * score_8 * score_9 * score_10 * score_11 * score_12 * score_13 * score_14 * score_15 * score_16 * score_17 * score_18 * score_19 * score_20 * score_21 * score_22 * score_23 * score_24 * score_25\n",
        "        total_acc = total_correct/total_train\n",
        "        lastest_train_acc.append(total_acc)\n",
        "        print(\"Model Score = \", total_acc*score) \n",
        "        if curr_model_score < (total_acc*score):\n",
        "            curr_model_score = total_acc*score \n",
        "            print(\"Model Checkpoint saved!\")\n",
        "            torch.save({'model_state_dict': model.state_dict()}, '/content/drive/MyDrive/EEC205/resnet18_best.pt')\n",
        "\n",
        "        epoch_count = epoch_count + 1\n",
        "\n",
        "        if epoch_count % 5 == 0:\n",
        "          print(\"Tenth Model Checkpoint saved!\")\n",
        "          torch.save({'model_state_dict': model.state_dict()}, '/content/drive/MyDrive/EEC205/resnet18_tenth.pt')\n",
        "\n",
        "        if epoch_count == num_epochs - 1:\n",
        "          print(\"Final Model Checkpoint saved!\")\n",
        "          #torch.save({'model_state_dict': model.state_dict()}, PATH.split('.')[0]+'_final'+PATH.split('.')[1])\n",
        "          torch.save({'model_state_dict': model.state_dict()}, '/content/drive/MyDrive/EEC205/resnet18_final.pt')\n",
        "\n",
        "    return all_losses, loss_epoch, lastest_train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO4ROpAnnPHO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VULVL64jnPEs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjNPKk79nPB5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK_aaApunOlX"
      },
      "outputs": [],
      "source": [
        "transforms_all = transforms.Compose([\n",
        "    transforms.Resize((300,300)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transforms_seg = transforms.Compose([\n",
        "    transforms.Resize((300,300)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgi5HIcnTIfe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class_dataset = PreTrain_Dataset('/content/drive/MyDrive/EEC205/dataset/',transform=transforms_all,isSeg=False)\n",
        "\n",
        "seg_dataset = PreTrain_Dataset('/content/drive/MyDrive/EEC205/dataset/',transform=transforms_all,transform_gt=transforms_seg,isSeg=True)\n",
        "\n",
        "mend_dataset = PreMendeleyTrain_Dataset(csv_file='/content/drive/MyDrive/EEC205/leaves_label.csv', root_dir='/content/drive/MyDrive/EEC205/hb74ynkjcn-1/', transform=transforms_all)\n",
        "\n",
        "val_bali = torchvision.datasets.ImageFolder('/content/drive/MyDrive/EEC205/data/val',transform=transforms_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtQr1AHoj8aG"
      },
      "outputs": [],
      "source": [
        "bsize = 200\n",
        "\n",
        "\n",
        "classloader = DataLoader(class_dataset, batch_size=bsize,\n",
        "                       shuffle=True, num_workers=0, drop_last = True)\n",
        "\n",
        "segloader = DataLoader(seg_dataset, batch_size=bsize,\n",
        "                       shuffle=True, num_workers=0, drop_last = True)\n",
        "\n",
        "mendloader = DataLoader(mend_dataset, batch_size=bsize,\n",
        "                       shuffle=True, num_workers=0, drop_last = True)\n",
        "\n",
        "valloader = DataLoader(val_bali, batch_size=bsize,\n",
        "                        shuffle=False, num_workers=0, drop_last = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyQA6lr-mBQ3"
      },
      "outputs": [],
      "source": [
        "save_model_path = '/content/drive/MyDrive/EEC205/resnet18.pt'\n",
        "\n",
        "#model = Leaf_FCN()\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "model.fc = nn.Linear(512,185)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "loss1 = nn.CrossEntropyLoss()\n",
        "loss2 = nn.MSELoss()\n",
        "loss3 = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZlbWA8pmBjX",
        "outputId": "2d8aadd6-5b9f-4d67-8ded-5b0095576578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Randomly Initialized\n",
            "Epoch [1/20], Step [5/146],  Loss: 2.7011\n",
            "Epoch [1/20], Step [10/146],  Loss: 2.1546\n",
            "Epoch [1/20], Step [15/146],  Loss: 1.9317\n",
            "Epoch [1/20], Step [20/146],  Loss: 1.4434\n",
            "Epoch [1/20], Step [25/146],  Loss: 1.2672\n",
            "Epoch [1/20], Step [30/146],  Loss: 1.1037\n",
            "Epoch [1/20], Step [35/146],  Loss: 0.8996\n",
            "Epoch [1/20], Step [40/146],  Loss: 0.8757\n",
            "Epoch [1/20], Step [45/146],  Loss: 0.7156\n",
            "Epoch [1/20], Step [50/146],  Loss: 0.4726\n",
            "Epoch [1/20], Step [55/146],  Loss: 0.6084\n",
            "Epoch [1/20], Step [60/146],  Loss: 0.4389\n",
            "Epoch [1/20], Step [65/146],  Loss: 0.4317\n",
            "Epoch [1/20], Step [70/146],  Loss: 0.3943\n",
            "Epoch [1/20], Step [75/146],  Loss: 0.2721\n",
            "Epoch [1/20], Step [80/146],  Loss: 0.3745\n",
            "Epoch [1/20], Step [85/146],  Loss: 0.2922\n",
            "Epoch [1/20], Step [90/146],  Loss: 0.2928\n",
            "Epoch [1/20], Step [95/146],  Loss: 0.2307\n",
            "Epoch [1/20], Step [100/146],  Loss: 0.2023\n",
            "Epoch [1/20], Step [105/146],  Loss: 0.2018\n",
            "Epoch [1/20], Step [110/146],  Loss: 0.2549\n",
            "Epoch [1/20], Step [115/146],  Loss: 0.1462\n",
            "Epoch [1/20], Step [120/146],  Loss: 0.1384\n",
            "Epoch [1/20], Step [125/146],  Loss: 0.1712\n",
            "Epoch [1/20], Step [130/146],  Loss: 0.1180\n",
            "Epoch [1/20], Step [135/146],  Loss: 0.1077\n",
            "Epoch [1/20], Step [140/146],  Loss: 0.1482\n",
            "Epoch [1/20], Step [145/146],  Loss: 0.0986\n",
            "END OF EPOCH\n",
            "Model Score =  0.35625365839222256\n",
            "Model Checkpoint saved!\n",
            "Epoch [2/20], Step [5/146],  Loss: 0.1079\n",
            "Epoch [2/20], Step [10/146],  Loss: 0.0797\n",
            "Epoch [2/20], Step [15/146],  Loss: 0.0974\n",
            "Epoch [2/20], Step [20/146],  Loss: 0.0509\n",
            "Epoch [2/20], Step [25/146],  Loss: 0.0828\n",
            "Epoch [2/20], Step [30/146],  Loss: 0.0669\n",
            "Epoch [2/20], Step [35/146],  Loss: 0.0904\n",
            "Epoch [2/20], Step [40/146],  Loss: 0.0568\n",
            "Epoch [2/20], Step [45/146],  Loss: 0.0343\n",
            "Epoch [2/20], Step [50/146],  Loss: 0.0516\n",
            "Epoch [2/20], Step [55/146],  Loss: 0.0537\n",
            "Epoch [2/20], Step [60/146],  Loss: 0.0517\n",
            "Epoch [2/20], Step [65/146],  Loss: 0.0617\n",
            "Epoch [2/20], Step [70/146],  Loss: 0.0288\n",
            "Epoch [2/20], Step [75/146],  Loss: 0.0485\n",
            "Epoch [2/20], Step [80/146],  Loss: 0.0462\n",
            "Epoch [2/20], Step [85/146],  Loss: 0.0430\n",
            "Epoch [2/20], Step [90/146],  Loss: 0.0362\n",
            "Epoch [2/20], Step [95/146],  Loss: 0.0531\n",
            "Epoch [2/20], Step [100/146],  Loss: 0.0455\n",
            "Epoch [2/20], Step [105/146],  Loss: 0.0323\n",
            "Epoch [2/20], Step [110/146],  Loss: 0.0441\n",
            "Epoch [2/20], Step [115/146],  Loss: 0.0420\n",
            "Epoch [2/20], Step [120/146],  Loss: 0.0386\n",
            "Epoch [2/20], Step [125/146],  Loss: 0.0351\n",
            "Epoch [2/20], Step [130/146],  Loss: 0.0274\n",
            "Epoch [2/20], Step [135/146],  Loss: 0.0498\n",
            "Epoch [2/20], Step [140/146],  Loss: 0.0410\n",
            "Epoch [2/20], Step [145/146],  Loss: 0.0253\n",
            "END OF EPOCH\n",
            "Model Score =  0.7956325298742897\n",
            "Model Checkpoint saved!\n",
            "Epoch [3/20], Step [5/146],  Loss: 0.0233\n",
            "Epoch [3/20], Step [10/146],  Loss: 0.0156\n",
            "Epoch [3/20], Step [15/146],  Loss: 0.0094\n",
            "Epoch [3/20], Step [20/146],  Loss: 0.0201\n",
            "Epoch [3/20], Step [25/146],  Loss: 0.0175\n",
            "Epoch [3/20], Step [30/146],  Loss: 0.0090\n",
            "Epoch [3/20], Step [35/146],  Loss: 0.0169\n",
            "Epoch [3/20], Step [40/146],  Loss: 0.0152\n",
            "Epoch [3/20], Step [45/146],  Loss: 0.0153\n",
            "Epoch [3/20], Step [50/146],  Loss: 0.0145\n",
            "Epoch [3/20], Step [55/146],  Loss: 0.0134\n",
            "Epoch [3/20], Step [60/146],  Loss: 0.0251\n",
            "Epoch [3/20], Step [65/146],  Loss: 0.0134\n",
            "Epoch [3/20], Step [70/146],  Loss: 0.0448\n",
            "Epoch [3/20], Step [75/146],  Loss: 0.0185\n",
            "Epoch [3/20], Step [80/146],  Loss: 0.0163\n",
            "Epoch [3/20], Step [85/146],  Loss: 0.0107\n",
            "Epoch [3/20], Step [90/146],  Loss: 0.0201\n",
            "Epoch [3/20], Step [95/146],  Loss: 0.0092\n",
            "Epoch [3/20], Step [100/146],  Loss: 0.0079\n",
            "Epoch [3/20], Step [105/146],  Loss: 0.0201\n",
            "Epoch [3/20], Step [110/146],  Loss: 0.0109\n",
            "Epoch [3/20], Step [115/146],  Loss: 0.0090\n",
            "Epoch [3/20], Step [120/146],  Loss: 0.0304\n",
            "Epoch [3/20], Step [125/146],  Loss: 0.0264\n",
            "Epoch [3/20], Step [130/146],  Loss: 0.0164\n",
            "Epoch [3/20], Step [135/146],  Loss: 0.0415\n",
            "Epoch [3/20], Step [140/146],  Loss: 0.0211\n",
            "Epoch [3/20], Step [145/146],  Loss: 0.0139\n",
            "END OF EPOCH\n",
            "Model Score =  0.9266638548416319\n",
            "Model Checkpoint saved!\n",
            "Epoch [4/20], Step [5/146],  Loss: 0.0147\n",
            "Epoch [4/20], Step [10/146],  Loss: 0.0196\n",
            "Epoch [4/20], Step [15/146],  Loss: 0.0139\n",
            "Epoch [4/20], Step [20/146],  Loss: 0.0068\n",
            "Epoch [4/20], Step [25/146],  Loss: 0.0218\n",
            "Epoch [4/20], Step [30/146],  Loss: 0.0125\n",
            "Epoch [4/20], Step [35/146],  Loss: 0.0143\n",
            "Epoch [4/20], Step [40/146],  Loss: 0.0099\n",
            "Epoch [4/20], Step [45/146],  Loss: 0.0243\n",
            "Epoch [4/20], Step [50/146],  Loss: 0.1198\n",
            "Epoch [4/20], Step [55/146],  Loss: 0.0271\n",
            "Epoch [4/20], Step [60/146],  Loss: 0.0474\n",
            "Epoch [4/20], Step [65/146],  Loss: 0.0159\n",
            "Epoch [4/20], Step [70/146],  Loss: 0.0170\n",
            "Epoch [4/20], Step [75/146],  Loss: 0.0115\n",
            "Epoch [4/20], Step [80/146],  Loss: 0.0405\n",
            "Epoch [4/20], Step [85/146],  Loss: 0.0225\n",
            "Epoch [4/20], Step [90/146],  Loss: 0.0462\n",
            "Epoch [4/20], Step [95/146],  Loss: 0.0203\n",
            "Epoch [4/20], Step [100/146],  Loss: 0.0231\n",
            "Epoch [4/20], Step [105/146],  Loss: 0.0157\n",
            "Epoch [4/20], Step [110/146],  Loss: 0.0299\n",
            "Epoch [4/20], Step [115/146],  Loss: 0.0168\n",
            "Epoch [4/20], Step [120/146],  Loss: 0.0197\n",
            "Epoch [4/20], Step [125/146],  Loss: 0.0085\n",
            "Epoch [4/20], Step [130/146],  Loss: 0.0198\n",
            "Epoch [4/20], Step [135/146],  Loss: 0.0169\n",
            "Epoch [4/20], Step [140/146],  Loss: 0.0120\n",
            "Epoch [4/20], Step [145/146],  Loss: 0.0070\n",
            "END OF EPOCH\n",
            "Model Score =  0.9112217667885009\n",
            "Epoch [5/20], Step [5/146],  Loss: 0.0316\n",
            "Epoch [5/20], Step [10/146],  Loss: 0.0133\n",
            "Epoch [5/20], Step [15/146],  Loss: 0.0441\n",
            "Epoch [5/20], Step [20/146],  Loss: 0.0070\n",
            "Epoch [5/20], Step [25/146],  Loss: 0.0074\n",
            "Epoch [5/20], Step [30/146],  Loss: 0.0064\n",
            "Epoch [5/20], Step [35/146],  Loss: 0.0045\n",
            "Epoch [5/20], Step [40/146],  Loss: 0.0017\n",
            "Epoch [5/20], Step [45/146],  Loss: 0.0116\n",
            "Epoch [5/20], Step [50/146],  Loss: 0.0052\n",
            "Epoch [5/20], Step [55/146],  Loss: 0.0037\n",
            "Epoch [5/20], Step [60/146],  Loss: 0.0043\n",
            "Epoch [5/20], Step [65/146],  Loss: 0.0052\n",
            "Epoch [5/20], Step [70/146],  Loss: 0.0024\n",
            "Epoch [5/20], Step [75/146],  Loss: 0.0057\n",
            "Epoch [5/20], Step [80/146],  Loss: 0.0110\n",
            "Epoch [5/20], Step [85/146],  Loss: 0.0107\n",
            "Epoch [5/20], Step [90/146],  Loss: 0.0028\n",
            "Epoch [5/20], Step [95/146],  Loss: 0.0171\n",
            "Epoch [5/20], Step [100/146],  Loss: 0.0296\n",
            "Epoch [5/20], Step [105/146],  Loss: 0.0242\n",
            "Epoch [5/20], Step [110/146],  Loss: 0.0161\n",
            "Epoch [5/20], Step [115/146],  Loss: 0.0057\n",
            "Epoch [5/20], Step [120/146],  Loss: 0.0096\n",
            "Epoch [5/20], Step [125/146],  Loss: 0.0137\n",
            "Epoch [5/20], Step [130/146],  Loss: 0.0063\n",
            "Epoch [5/20], Step [135/146],  Loss: 0.0174\n",
            "Epoch [5/20], Step [140/146],  Loss: 0.0154\n",
            "Epoch [5/20], Step [145/146],  Loss: 0.0198\n"
          ]
        }
      ],
      "source": [
        "all_losses, loss_epoch, lastest_train_acc = fit(model, loss1, loss2,loss3, optimizer, trainloader,segloader,mendloader, valloader, bsize, epochs, scheduler = None, stat_count=5, device=None,num_ch=3,stepsize=10,PATH = save_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoF9xVoTsTpF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tappwp0asTmo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4DpH5_MsTiv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c8PmzTEQv0R"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "EEC206PreTrain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}